# Make sure to rename this file to config.toml
# Otherwise, it won't be recognised
# Also, don't forget to place your API Token

# AUTOGENERATED SECTION - DO NOT EDIT
[chat.structure]
changelog_checksum = "dummy_value"

# Feel free to edit any value below this line
[chat.customizations]
use_emoji = true
fallback_char = "?"

[chat.defaults]
temperature = 1
system_role = "assistant"
model = "gpt3"
assistant = "anthropic-haiku"
assistant_role = "<instructions> You are an AI assistant that helps select the optimal AI model to answer a user questions. It is not expected you to answer or provide any additional explanation, only to pass the question in an optimized form further.You will receive a user question and you reply in a form of a JSON only, exaclty as provided in this example. No additional clarifications or explanation from your side are required!: <example> { \"model\": \"{{the_model_you_think_will_answer_properly_at_a_reasonable_cost}}\", \"messages\": [ { \"role\": \"system\", \"content\": \"{{the_system_instruction_for_the_targeted_model_written_by_you_based_on_the_nature_of_the_question.your_answer_or_opinion_should_not_be_included.}}\" }, { \"role\": \"user\", \"content\": \"{{optimized_token-efficient_version_of_the_original_user_question.}}\" } ] } } </example> When the user provides a question, analyze its complexity across key dimensions like topic (general knowledge, math, coding, etc.), depth of knowledge required, and length. Based on the complexity analysis, silently decide on the most suitable model from the following list, optimizing for query type, model capability rank, and price based on the following table1 and table2: <table1> | model_name | Input cost in USD per 1M tokens | Output cost in USD per 1M tokens | Rank | |------------|---------------------------------|----------------------------------|------| | anthropic-opus | 15.00 | 75.00 | 1 | | gpt4 | 10.00 | 30.00 | 1 | | anthropic-sonnet | 3.00 | 15.00 | 4 | | anthropic-haiku | 0.25 | 1.25 | 7 | </table1> The models are listed from best to worst. Always suggest only the best fitting model. Use the below table to analyze the query and provide the best fit: <table2> | Complexity Category | Description | Examples | |---------------------|-------------|----------| | 1. Extremely Complex | Challenging tasks requiring deep reasoning and specialized knowledge. | - Solving complex mathematical equations - Software debugging and optimization - Theoretical physics simulations | | 2. Scientific and Technical | Handling scientific research, engineering, and technical domains. | - Predicting protein structures - Simulating fluid dynamics - Natural language understanding for scientific literature | | 3. Real-World Applications | Practical applications impacting daily life. | - Autonomous driving systems - Medical diagnosis using radiological images - Financial risk assessment | | 4. Common Tasks and Productivity | Routine tasks enhancing productivity. | - Language translation - Image recognition in photo apps - Virtual assistants for scheduling and reminders | | 5. General Conversational Topics | Casual conversations and natural language understanding. | - Chatbots providing friendly responses - Social media bots interacting with users - Creative writing | </table2> Some additional guidelines: - anthropic-opus is preffered for jokes, philosophical, moral and engaging conversations, more suitable for mimicing human interactions. - gpt4 model is the most efficient for English and Python, to be preferred for complex programming, riddles, logical, reasoning and technical topics. - If your use cases is non-English, really worth looking at anthropic-* models. In your response, in the provided JSON model return only a semantically optimized, token-efficient version of the original question to send to the model. Create a custom system prompt that corresponds to the question. Provide your full response as the specified JSON format only, with absolutely no text or explanations in your reply outside of the specified JSON structure. </instructions> "

[chat.features]
model_selector = true
adjust_temperature = true
role_selector = true
save_chat_on_exit = true
continue_chat = true
debug = true
disable_intro_help_message = false
assistant_mode = true
ai_managed = false

[chat.roles]
ai_expert = "Simplify AI principles, Machine Learning, and Neural Networks for all understanding levels."
business_professional = "Propose focused strategies for market-driven sustainable business growth."
software_engineer = "Summarize best practices in coding for quality, optimization, and efficiency with applicable examples."
educator = "Customize advice for effective teaching and learning that caters to various styles."
fitness_coach = "Craft individualized fitness plans emphasizing balanced exercise, diet, and wellness."
healthcare_professional = "Provide health advice informed by the latest research, underlining smart health decision-making."
legal_professional = "Clarify legal issues and provide case-specific guidance for informed decision-making."
manager = "Share leadership insights to boost team efficiency, collaboration, and decision-making."
nutritionist = "Advise on nutrition blending science with practicality, focusing on proven, sustainable dietary habits."
scientist = "Make science engaging and understandable across math, physics, chemistry, and biology."
assistant = "Deliver precise and informative virtual assistance for any inquiry efficiently."

[chat.models.gpt3]
api_key = "YOUR_OPENAI_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.0005
model_max_tokens = 4096
model_name = "gpt-3.5-turbo"
model_output_pricing_per_1k = 0.0015

[chat.models.gpt4]
api_key = "YOUR_OPENAI_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.01
model_max_tokens = 4096
model_name = "gpt-4-turbo-preview"
model_output_pricing_per_1k = 0.03

[chat.models.gpt4-vision]
api_key = "YOUR_OPENAI_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.01
model_max_tokens = 4096
model_name = "gpt-4-vision-preview"
model_output_pricing_per_1k = 0.03

# [chat.models.mistral]
# api_key = "YOUR_MISTRALAI_API_KEY"
# api_usage = 0
# model_input_pricing_per_1k = 0.002
# model_max_tokens = 0
# model_name = "mistral-small-latest"
# model_output_pricing_per_1k = 0.006

[chat.models.mistral]
api_key = "YOUR_MISTRALAI_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.0027
model_max_tokens = 0
model_name = "mistral-medium-latest"
model_output_pricing_per_1k = 0.0081

# [chat.models.mistral]
# api_key = "YOUR_MISTRALAI_API_KEY"
# api_usage = 0
# model_input_pricing_per_1k = 0.008
# model_max_tokens = 0
# model_name = "mistral-large-latest"
# model_output_pricing_per_1k = 0.024

[chat.models.anthropic-haiku]
api_key = "YOUR_ANTHROPIC_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.00025
model_max_tokens = 4096
model_name = "claude-3-haiku-20240307"
model_output_pricing_per_1k = 0.00125

[chat.models.anthropic-sonnet]
api_key = "YOUR_ANTHROPIC_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.003
model_max_tokens = 4096
model_name = "claude-3-sonnet-20240229"
model_output_pricing_per_1k = 0.015

[chat.models.anthropic-opus]
api_key = "YOUR_ANTHROPIC_API_KEY"
api_usage = 0
model_input_pricing_per_1k = 0.015
model_max_tokens = 4096
model_name = "claude-3-opus-20240229"
model_output_pricing_per_1k = 0.075